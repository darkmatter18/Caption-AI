{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption AI\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pytz\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from torchvision import transforms\n",
    "from model import EncoderCNN, DecoderRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 256          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 1024           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "tz = 'Asia/Kolkata'\n",
    "localFormat = \"%Y-%m-%d %H:%M:%S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "transform_test = transforms.Compose([transforms.Resize(224),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                                          (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO location: /home/jupyter/Caption-AI/ImageCaptioning/COCODataset\n",
      "Loading train Images form: /home/jupyter/Caption-AI/ImageCaptioning/COCODataset/images/train2014\n",
      "Loading train Annotarions form: /home/jupyter/Caption-AI/ImageCaptioning/COCODataset/annotations/captions_train2014.json\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.71s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 822/414113 [00:00<00:50, 8217.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:42<00:00, 9850.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO location: /home/jupyter/Caption-AI/ImageCaptioning/COCODataset\n",
      "Loading val Images form: /home/jupyter/Caption-AI/ImageCaptioning/COCODataset/images/val2014\n",
      "Loading val Annotarions form: /home/jupyter/Caption-AI/ImageCaptioning/COCODataset/annotations/captions_val2014.json\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 903/202654 [00:00<00:22, 9021.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.29s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202654/202654 [00:20<00:00, 9654.70it/s] \n"
     ]
    }
   ],
   "source": [
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# Build val data loader.\n",
    "val_data_loader = get_loader(transform=transform_test,\n",
    "                         mode='val',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (embed): Embedding(9955, 1024)\n",
       "  (lstm): LSTM(1024, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=9955, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers=3, drop=0.5)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params=params, lr = 0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, captions = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "encoder_file = 'encoder-500.pkl'\n",
    "decoder_file = 'decoder-500.pkl'\n",
    "optim_file = 'optim-500.pkl'\n",
    "\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./modelsX', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./modelsX', decoder_file)))\n",
    "optimizer.load_state_dict(torch.load(os.path.join('./modelsX', optim_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-06-06 16:03:10] Epoch [21/25], Step [100/1618], Loss: 2.1424, Perplexity: 8.5195, Valid Loss: 2.2964\n",
      "Validation loss decreased (inf --> 2.296426).  Saving model ...\n",
      "[2020-06-06 16:23:45] Epoch [21/25], Step [200/1618], Loss: 2.1725, Perplexity: 8.7803, Valid Loss: 2.3142\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Adding the Previous epochs\n",
    "previous_epoch = 20\n",
    "num_epochs = 25\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "# Open the training log file.\n",
    "# f = open(log_file, 'w')\n",
    "# f.close()\n",
    "\n",
    "for epoch in range(1+previous_epoch, num_epochs+1):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "\n",
    "        decoder.train()\n",
    "        encoder.train()\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        nn.utils.clip_grad_norm_(params, 5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update training loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            \n",
    "            decoder.train()\n",
    "            encoder.train()\n",
    "\n",
    "            indices_v = val_data_loader.dataset.get_train_indices()\n",
    "            new_sampler_v = data.sampler.SubsetRandomSampler(indices=indices_v)\n",
    "            val_data_loader.batch_sampler.sampler = new_sampler_v\n",
    "            \n",
    "            images, captions = next(iter(val_data_loader))\n",
    "                \n",
    "            # Move batch of images and captions to GPU if CUDA is available.\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "                \n",
    "            # Pass the inputs through the CNN-RNN model.\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions)\n",
    "                \n",
    "            # Calculate the batch loss.\n",
    "            loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "            \n",
    "            # Valid loss\n",
    "            valid_loss = loss.item()\n",
    "            \n",
    "            # Get training statistics.\n",
    "            _train_loss = train_loss / print_every\n",
    "            _p_train_loss = np.exp(_train_loss)\n",
    "            \n",
    "            #Get current time\n",
    "            _t_now = datetime.utcnow().replace(tzinfo=pytz.utc).astimezone(pytz.timezone(tz)).strftime(localFormat)\n",
    "            \n",
    "            stats = '[%s] Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f, Valid Loss: %.4f' % (_t_now, epoch, num_epochs, i_step, total_step,  _train_loss, _p_train_loss, valid_loss)\n",
    "            \n",
    "            # Print and Reset\n",
    "            print(stats)\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            # save model if validation loss has decreased\n",
    "            if valid_loss <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                    valid_loss_min,\n",
    "                    valid_loss))\n",
    "                torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % i_step))\n",
    "                torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % i_step))\n",
    "                # saving the optimizer for future use\n",
    "                torch.save(optimizer.state_dict(), os.path.join('./models', 'optim-%d.pkl' % i_step))\n",
    "                valid_loss_min = valid_loss\n",
    "            \n",
    "            # Append log\n",
    "            with open(log_file, 'a+') as f:\n",
    "                f.write(stats + '\\n')\n",
    "            \n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "        # saving the optimizer for future use\n",
    "        torch.save(optimizer.state_dict(), os.path.join('./models', 'optim-%d.pkl' % epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
